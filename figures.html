<pre>
library(ggplot2)
library(rjags)
library(MCMCpack)
library(coda)
library(LaplacesDemon)
library(mixtools)
library(gridExtra)
library(readxl)
library(MASS)
library(tidyverse)
library(tidybayes)
require(mvtnorm)
library(gganimate)
library(gifski)
library(fields)
library(tidyr)
library(plotly)
library(purrr)

######### Algoritmo DP #########

set.seed(123)
data = rnorm(15, 3, 1)
hist(data, breaks = 15, prob = T)
lines(density(data))
stick_breaking = function(K, alpha) {
  V = numeric(K)
  b = 1
  for(i in c(1:(K-1))){
    a = rbeta(1, 1, alpha) 
    V[i] = a * b
    b = b * (1 - a)
  }
  V[K] = b # Ver esto
  return(V)
}

F_t = function(t, S, V) {
  sum(V[S <= t])
}

alpha = 1
K = 10000
n.iter = 20

t_values = seq(-6, 6, by = 0.1)  # Ajusta el paso según sea necesario
F_values_matrix = matrix(0, nrow = length(t_values), ncol = n.iter)

for(i in c(1:n.iter)) {
  V = stick_breaking(K,alpha)
  S = rnorm(K,0,1) # Creencia F_0
  F_values_matrix[,i] = sapply(t_values, F_t, S = S, V = V)
  df = as.matrix(data.frame(S,V))
}
sum(V)
density_values = dnorm(t_values, mean = 0, sd = 1)

plot(df, type = "h", lwd = 2, col = "blue", xlab = "x", ylab = "y", ylim = c(0,0.4))
points(df, pch = 16, col = "blue")
lines(t_values, density_values, type = "l", main = "Densidad de la Normal Estándar", xlab = "t", ylab = "Densidad")

barplot(V[1:4], beside = TRUE, names.arg = paste("Componente", 1:4),
        ylim = c(0,1),
        xlab = "Componente", ylab = "Peso",
        main = "Pesos V de los primeros 4 componentes", col = "blue")


plot(t_values, F_values_matrix[,1], type = "s", col = "grey", lwd = 2, 
     xlab = "t", ylab = "F(t)", 
     main = "Función de Distribución Acumulada F(t) vs Normal Estándar")

for(i in c(1:n.iter)) {
  lines(t_values, F_values_matrix[, i], col = "grey", lwd = 2, type = "s")
}

grid()

F_0 = pnorm(t_values, mean = 0, sd = 1)
lines(t_values, F_0, col = "red", lwd = 2)

legend("bottomright", legend = "F_0", 
       col = "red", lty = 1, lwd = 2)
text(x = -3, y = 0.95, labels = paste("alpha =", alpha), cex = 1.2, col = "black")

######### Posteriori #########

n.iter = 20
ecdf_data = ecdf(data)
F_hat = ecdf_data(t_values)
n = length(data)
F_posterior_values_matrix = matrix(0, nrow = length(t_values), ncol = n.iter)

F_bar = (n/(n+alpha))*F_hat + (alpha/(n+alpha))*F_0

for(i in c(1:n.iter)) {
  V = stick_breaking(K,alpha+n)
  sampled_values = numeric(K)
  for (j in 1:K) {
    u = runif(1)
    if (u < (n / (n + alpha))) {
      idx = sample(1:length(F_hat), 1, prob = diff(c(0, F_hat))) 
      sampled_values[j] = t_values[idx]
    } else {
      sampled_values[j] = rnorm(1, 0, 1)
    }
  }
  F_posterior_values_matrix[,i] = sapply(t_values, F_t, S = sampled_values, V = V)
}

plot(t_values, F_posterior_values_matrix[,1], type = "s", col = "grey", lty = 2, lwd = 2, 
     xlab = "t", ylab = "F(t)", 
     main = "Función de Distribución Acumulada F(t) vs Normal(3,1)")

for(i in c(1:n.iter)) {
  lines(t_values, F_posterior_values_matrix[, i], col = "grey", lwd = 2, type = "s")
}

grid()

normal_cdf = pnorm(t_values, mean = 3, sd = 1)
lines(t_values, normal_cdf, col = "red", lwd = 2)

text(x = -3, y = 0.95, labels = paste("alpha =", alpha), cex = 1.2, col = "black")

F_mean = rowMeans(F_posterior_values_matrix)

lines(t_values, F_mean, col = "blue", lwd = 2, type = "s")
lines(t_values, F_hat, col = "green", lwd = 2, type = "s")

legend("bottomright", legend = c("F_mean","F_hat"), 
       col = c("blue","green"), lty = 1, lwd = 2)

######### Algoritmo DPM #########

set.seed(123)
data = c(rnorm(25,-2,1), rnorm(25,2,1))
plot(data, pch = 19)
hist(data, breaks = 15, prob = T)
lines(density(data))
stick_breaking = function(K, alpha) {
  V = numeric(K)
  b = 1
  for(i in c(1:K)){
    a = rbeta(1, 1, alpha) 
    V[i] = a * b
    b = b * (1 - a)
  }
  return(V)
}

F_t = function(t, S, V) {
  sum(V[S <= t])
}

alpha = 1
K = 10000
n.iter = 20

t_values = seq(-6, 6, by = 0.1)  # Ajusta el paso según sea necesario
F_values_matrix = matrix(0, nrow = length(t_values), ncol = n.iter)

for(i in c(1:n.iter)) {
  V = stick_breaking(K,alpha)
  S = rnorm(K,0,1) # Creencia F_0
  F_values_matrix[,i] = sapply(t_values, F_t, S = S, V = V)
}

plot(t_values, F_values_matrix[,1], type = "s", col = "grey", lwd = 2, 
     xlab = "t", ylab = "F(t)", 
     main = "Función de Distribución Acumulada F(t) vs Normal Estándar")

for(i in c(1:n.iter)) {
  lines(t_values, F_values_matrix[, i], col = "grey", lwd = 2, type = "s")
}

grid()

F_0 = pnorm(t_values, mean = 0, sd = 1)
lines(t_values, F_0, col = "red", lwd = 2)


legend("bottomright", legend = "F_0", 
       col = "red", lty = 1, lwd = 2)
text(x = -3, y = 0.95, labels = paste("alpha =", alpha), cex = 1.2, col = "black")


sample_from_F_first_curve = function(F_values, t_values) {
  u = runif(1) 
  t_sample = t_values[which.min(abs(F_values - u))]
  return(t_sample)
}

sampled_value_first_curve = matrix(0, nrow = K, ncol = n.iter)

for (i in c(1:n.iter)) {
  for (j in c(1:K)) {
    sampled_value_first_curve[j,i] = sample_from_F_first_curve(F_values_matrix[,i], t_values)
  }
}

x_n = matrix(0, nrow = K, ncol = n.iter)
for (i in c(1:n.iter)) {
  x_n[,i] = rnorm(K,sampled_value_first_curve[,i], sd = 1)
}

plot(density(x_n[,1]), col = "grey", lwd = 2, type = "s", lty = 2)

for(i in c(2:n.iter)) {
  lines(density(x_n[,i]), col = "grey", lwd = 2, type = "s", lty = 2)
}

grid()

######### Posteriori #########

n.iter = 20
ecdf_data = ecdf(data)
F_hat = ecdf_data(t_values)
n = length(data)
F_posterior_values_matrix = matrix(0, nrow = length(t_values), ncol = n.iter)

F_bar = (n/(n+alpha))*F_hat + (alpha/(n+alpha))*F_0

for(i in c(1:n.iter)) {
  V = stick_breaking(K,alpha+n)
  sampled_values = numeric(K)
  for (j in 1:K) {
    u = runif(1) 
    if (u < (n / (n + alpha))) {
      idx = sample(1:length(F_hat), 1, prob = diff(c(0, F_hat)))
      sampled_values[j] = t_values[idx]
    } else {
      sampled_values[j] = rnorm(1, 0, 1)
    }
  }
  F_posterior_values_matrix[,i] = sapply(t_values, F_t, S = sampled_values, V = V)
}

plot(t_values, F_posterior_values_matrix[,1], type = "s", col = "grey", lty = 2, lwd = 2, 
     xlab = "t", ylab = "F(t)", 
     main = "Función de Distribución Acumulada F(t) vs Mezcla de Normales")

for(i in c(1:n.iter)) {
  lines(t_values, F_posterior_values_matrix[, i], col = "grey", lwd = 2, type = "s")
}

grid()

normal_cdf = (1/2)*pnorm(t_values, mean = -2, sd = 1) + 
  (1/2)*pnorm(t_values, mean = 2, sd = 1) 
lines(t_values, normal_cdf, col = "red", lwd = 2)

text(x = -3, y = 0.95, labels = paste("alpha =", alpha), cex = 1.2, col = "black")

F_mean = rowMeans(F_posterior_values_matrix)

lines(t_values, F_mean, col = "blue", lwd = 2, type = "s")
lines(t_values, F_hat, col = "green", lwd = 2, type = "s")

sampled_value_first_curve = matrix(0, nrow = K, ncol = n.iter)

for (i in c(1:n.iter)) {
  for (j in c(1:K)) {
    sampled_value_first_curve[j,i] = sample_from_F_first_curve(F_posterior_values_matrix[,i], t_values)
  }
}

x_n = matrix(0, nrow = K, ncol = n.iter)
for (i in c(1:n.iter)) {
  x_n[,i] = rnorm(K,sampled_value_first_curve[,i], sd = 1)
}

####

densidades = apply(x_n, 2, density)

x_vals = densidades[[1]]$x

densidades_vals = sapply(densidades, function(d) approxfun(d$x, d$y)(x_vals))

densidades_vals_clean = densidades_vals[, !apply(is.na(densidades_vals), 2, any)]

densidad_promedio = rowMeans(densidades_vals_clean)

percentil_2_5 = apply(densidades_vals_clean, 1, function(x) quantile(x, 0.025))
percentil_97_5 = apply(densidades_vals_clean, 1, function(x) quantile(x, 0.975))

### Ploteando ###

plot(density(x_n[,1]), col = "grey", lwSd = 2, type = "s", lty = 2, ylim = c(0,0.17))

for(i in c(2:n.iter)) {
  lines(density(x_n[,i]), col = "grey", lwd = 2, type = "s", lty = 2)
}
lines(density(data), col = "blue", lwd = 2, type = "s")
lines(x_vals, densidad_promedio, col='red', lwd=2) 

polygon(c(x_vals, rev(x_vals)), c(percentil_2_5, rev(percentil_97_5)), 
        col=rgb(0, 0, 1, 0.3), border=NA) 

############## PROCESO GAUSSIANO ##############

### Datos

set.seed(1)
x_values = sort(sample(1:500,100))
x = x_values/100

d = fields::rdist(x)
phi = 1.2
sigma_g = 2
K = sigma_g^2*exp(-(phi^2)*(d^2))

g = mvtnorm::rmvnorm(5, sigma=K)
matplot(x, t(g), 
        type="l", 
        ylab="y")

sigma_y <- 0.03
eps <- rnorm(100,0,sigma_y) # random noise
g <- mvtnorm::rmvnorm(1, sigma=K) # GP
y <- c(g) + eps # Simulated data = GP + random noise

data = data.frame(x_values = x_values,
             x = x,
             y = y, 
             gp = c(g))

ggplot(data, aes(x = x_values, y = y)) +
  geom_point(aes(colour = "Datos simulados")) +
  geom_line(aes(x = x_values, y = gp, colour = "Proceso Gaussiano")) +
  labs(colour = "")

model_string <- '
model{

  gp ~ dmnorm(mu,Sigma.inv)
  Sigma.inv <- inverse(Sigma)
  
  for(i in 1:n_obs)
  {
    mu[i] <- 0
    Sigma[i,i] <- sigma_g^2 + 0.00001
    for(j in (i+1):n_obs) {
    Sigma[i,j] <- sigma_g^2*exp(-(phi^2)*(d[i,j]^2))
    Sigma[j,i] <- Sigma[i,j]
    }
      
    y[i]~dnorm(gp[i],sigma_y^-2)
  }
  
  sigma_g ~ dt(0,10^-2,1)T(0,)
  phi ~ dt(0,4^-2,1)T(0,)
  sigma_y ~ dt(0,10^-2,1)T(0,)
}

'

x <- data$x
y <- data$y
n_obs <- length(x)
dist <- rdist(x) 

data_list <- list(x = x, y = y, n_obs = n_obs, d = dist)

set.seed(1)
train_index <- sample(1:n_obs, size = 0.8 * n_obs)

data_train <- list(
  y = data_list$y[train_index],
  n_obs = length(train_index),
  d = data_list$d[train_index, train_index]
)
x_train = data_list$x[train_index]

data_test <- list(
  y = data_list$y[-train_index],
  n_obs = length(data_list$x) - length(train_index),
  d = data_list$d[-train_index, -train_index]
)
x_test = data_list$x[-train_index]

params = c("sigma_g", "phi", "sigma_y")  

inits1 = list(.RNG.name = "base::Mersenne-Twister", .RNG.seed = 1)
inits2 = list(.RNG.name = "base::Mersenne-Twister", .RNG.seed = 2)
inits3 = list(.RNG.name = "base::Mersenne-Twister", .RNG.seed = 3)
mod = jags.model(textConnection(model_string), data = data_train, n.chains = 3,
                 inits = list(inits1, inits2, inits3))
update(mod, 1000)

mod_sim = coda.samples(model = mod, variable.names = params, n.iter = 10000)
mod_csim = as.mcmc(do.call(rbind, mod_sim))

summary(mod_csim)
gelman.diag(mod_sim)
?effectiveSize
autocorr.diag(mod_sim)
effectiveSize(mod_sim)

plot(mod_csim)

########### Predicciones ###########

n_pred = 20
x_star = seq(0,5, length.out = 20)


par_est = colMeans(mod_csim)
par_est = data.frame(phi = par_est[1], sigma_g = par_est[2], sigma_y = par_est[3])

Sigma <-  par_est$sigma_y*2 * diag(n_obs) + par_est$sigma_g^2*exp(-(par_est$phi^2)*rdist(x,x)^2)
Sigma_star <- par_est$sigma_g^2*exp(-(par_est$phi^2)*rdist(x_star,x)^2) 
Sigma_star_star <- par_est$sigma_g^2*exp(-(par_est$phi^2)*rdist(x_star,x_star)^2)

pred_mean <- Sigma_star %*% solve(Sigma, y)
pred_var <- Sigma_star_star - Sigma_star %*% solve(Sigma, t(Sigma_star))

pred_res <- tibble(pred_mean = pred_mean, 
                   x_values = x_star,
                   lwr_95 = pred_mean - 1.96 * sqrt(diag(pred_var)), 
                   upr_95 = pred_mean + 1.96 * sqrt(diag(pred_var)))

ggplot(pred_res, aes(x = x_values, y = pred_mean)) +
  geom_line() +
  geom_ribbon(aes(ymin = lwr_95, ymax = upr_95), alpha = 0.3) +
  geom_point(data = data, aes(x = x, y = y), alpha = 0.2) +
  labs(y = "y", colour = "", fill = "") +
  theme_bw() 

############ Agregar un solo punto ###########

new_point = 0.5  # Un nuevo punto de entrada (x)
new_observation <- pred_mean[which.min(abs(x_star - new_point))];new_observation 
new_point_var <- pred_var[which.min(abs(x_star - new_point)), which.min(abs(x_star - new_point))]

ggplot(pred_res, aes(x = x_values, y = pred_mean)) +
  geom_line() +
  geom_ribbon(aes(ymin = lwr_95, ymax = upr_95), alpha = 0.3) +
  geom_point(data = data, aes(x = x, y = y), alpha = 0.2) +
  annotate("point", x = new_point, y = new_observation, color = "red", size = 3) +  # Usar annotate() para el nuevo punto
  annotate("text", x = new_point, y = new_observation, label = paste("(", round(new_point, 2), ",", round(new_observation, 2), ")", sep=""), vjust = -1) +  # Etiqueta del nuevo punto
  labs(y = "y", colour = "", fill = "") +
  theme_bw()


set.seed(1)
simulated_values <- rnorm(1000, mean = new_observation, sd = sqrt(new_point_var))

lower_95 <- new_observation - 1.96 * sqrt(new_point_var)
upper_95 <- new_observation + 1.96 * sqrt(new_point_var)

ggplot() +
  geom_density(aes(x = simulated_values), fill = "blue", alpha = 0.5) +  # Distribución simulada
  geom_vline(xintercept = new_observation, color = "skyblue", linetype = "dashed", size = 1) +  # Media
  geom_vline(xintercept = lower_95, color = "red", linetype = "dashed", size = 1) +  # Límite inferior 95%
  geom_vline(xintercept = upper_95, color = "red", linetype = "dashed", size = 1) +  # Límite superior 95%
  annotate("text", x = lower_95, y = 0.05, label = paste("Lower 95%: ", round(lower_95, 2)), color = "green", vjust = -1) +  # Etiqueta inferior
  annotate("text", x = upper_95, y = 0.05, label = paste("Upper 95%: ", round(upper_95, 2)), color = "green", vjust = -1) +  # Etiqueta superior
  annotate("text", x = new_observation, y = 0.1, label = paste("Mean: ", round(new_observation, 2)), color = "red", vjust = -1) +  # Etiqueta media
  labs(title = "Distribución Condicional en el Punto con Intervalo de Confianza al 95%",
       x = "Valor predicho", y = "Densidad") +
  theme_bw()

########### Distribución de un punto ##########

point_indices <- seq(1, n_pred) 

anim_data <- data.frame()

x_range <- seq(-3, 1, by = 0.01)
for (point_index in point_indices) {
  mean_point <- pred_mean[point_index]
  var_point <- pred_var[point_index, point_index]
  sd_point <- sqrt(var_point)
  
  density_values <- dnorm(x_range, mean = mean_point, sd = sd_point)
  
  anim_data <- rbind(anim_data, data.frame(x = x_range, density = density_values, point_index = point_index, x_test = x_star[point_index]))
}

anim <- ggplot(anim_data, aes(x = x, y = density, group = point_index)) +
  # Usar densidad kernel usando dnorm() en lugar de histogramas
  geom_line(color = "blue", size = 1) +
  geom_vline(aes(xintercept = pred_mean[point_index]), color = "red", linetype = "dashed", size = 1) +
  labs(
    title = "Distribución de los datos de testeo",
    x = "Valor de y", y = "Densidad"
  ) +
  theme_minimal() +
  transition_states(point_index, transition_length = 2, state_length = 1) +  # Animación por `point_index`
  enter_fade() + exit_fade()

anim_save("animation2.gif", animation = anim, renderer = gifski_renderer())


#######################################################

##### Full conditionals.

# locations
theta_j <- function(f_0, k_theta, old_s, old_u, old_w, new_s){
  Ns <- apply(old_w, 2, cumsum)
  
  
  
  cuantos <- which.max(s_ik)
  probas <- sapply(cuantos,)
}


#### Auxiliares

q_0k <- function(v) (phi*M*(1-v)^(M-1))/beta(1+n_jk, M+m_jk)

q_ik <- function(u, v, w) (1-phi)*(1-u)^(M-1)*v^n_jk*(1-w)^m_jk

# Calcula la N nueva, todo es paralelizado por rapidez
new_N <- function(ws, us){
  cl <- makeCluster(detectCores())
  registerDoParallel(cl)
  
  cuantos <- nrow(ws)
  ws <- apply(ws, 2, cumsum)
  ws <- as.vector(ws); us <- 1 - as.vector(us)
  res <- foreach(i = 1:length(ws), .combine = 'c') %dopar% {
    ws[i] > ui[i]    
  }
  res <- matrix(res, nrow=cuantos)
  stopCluster(cl)
  max(apply(res, 2, function(x) which.max(x)))
}

######################################################

#######################################################
# Title: Dependent Generalised Dirichlet Process Priors
# Author: William Barcella
# Description: Main code
#######################################################

require(R2jags)
library(rjags)
library(R2jags)

# Source data
load("example_data_dgdp.RData")
TT <- dim(Y)[2] # Dimension of the response variable
n <- dim(Y)[1] # Number of patients 
K <-  20 # Truncation level for the DGDP

# Index variable for the risk group
Z <- cbind(rep(1,n),(lz-1))
uz <- unique(Z)

# Values to evaluate the predictive mean  
Xp1  <- as.vector(colMeans(X))
Xp2  <- as.vector(X[1,])
Xp3  <- as.vector(X[10,])

# Hyperparameter of G_0
m_int = c(0,0,0,0)
Tau_int = diag(4)*0.01


# Data to pass in JAGS
dataJags=list(y=Y, X=X, n=n, K=K, lz=lz, un=max(lz), 
              uz=uz, disT=disT, TT=TT,
              Xp1=Xp1,Xp2=Xp2,Xp3=Xp3, 
              m_int=m_int, Tau_int=Tau_int)

# Random initial values
#inits <-  c(list(list(.RNG.seed=2345)),list(list(.RNG.seed=1234)))
inits <-list(list(.RNG.seed=2345))

# JAGS script of the model
filein <- "example_code_dgdp.txt"

# Parameters to save
params <- c("inPred","bePred", "g",
            "yPred1","yPred2","yPred3", "coef","prsb","rho_omegaP","s_omega2P") 

fitmodel <- jags(model.file=filein, inits=inits, data=dataJags,
                 parameters.to.save=params, n.chains=1, n.iter=50000, n.burnin=30000,
                 n.thin=1, DIC=FALSE)

#######################################################
# Title: Dependent Generalised Dirichlet Process Priors
# Author: William Barcella
# Description: Posterior predictive means
#######################################################

### Package
require(coda)
require(mcmcplots)
require(ggmcmc)
require(MASS)
library(RColorBrewer)

par(mfrow=c(1,1),mar=c(2,2,2,2))


tim <- c(0,1,2,5)

# Source samples from the predictive
yPred1 <- fitmodel$BUGSoutput$sims.list$yPred1
yPred2 <- fitmodel$BUGSoutput$sims.list$yPred2
yPred3 <- fitmodel$BUGSoutput$sims.list$yPred3

#
traj1.1 <- colMeans(yPred1[,1,])
traj1.2 <- colMeans(yPred1[,2,])
plot(tim,traj1.1,type="o",ylim=c(1.8,10),cex.axis=0.7,main="",pch=1,lty=1)
lines(tim,traj1.2,type="o",pch=4,lty=1,col=2)

#
traj2.1 <- colMeans(yPred2[,1,])
traj2.2 <- colMeans(yPred2[,2,])
lines(tim,traj2.1,type="o",lty=2)
lines(tim,traj2.2,type="o",pch=4,col=2,lty=2)

#
traj3.1 <- colMeans(yPred3[,1,])
traj3.2 <- colMeans(yPred3[,2,])
lines(tim,traj3.1,type="o",lty=3)
lines(tim,traj3.2,type="o",pch=4,col=2,lty=3)

#
legend(3.3,10,cex=0.7,c("Group 1, x= 3.1, 3.8, 3.0, 3.3","Group 2, x= 3.1, 3.8, 3.0, 3.3",
                        "Group 1, x= 3.0, 0.0, 4.5, 3.7","Group 2, x= 3.0, 0.0, 4.5, 3.7",
                        "Group 1, x= 2.5, 1.9, 2.9, 3.5","Group 2, x= 2.5, 1.9, 2.9, 3.5"), 
       lty=c(1,1,2,2,3,3),
       lwd=c(1.5,1.5),col=c(1,2), bty="n", pch=c(1,4,1,4,1,4))

#######################################################
# Title: Dependent Generalised Dirichlet Process Priors
# Author: William Barcella
# Description: Marginal posterior predictive density 
#######################################################

par(mfrow=c(1,1),mar=c(2,2,2,2))

# Source samples from the predictive
yPred1 <- fitmodel$BUGSoutput$sims.list$yPred1
yPred2 <- fitmodel$BUGSoutput$sims.list$yPred2
yPred3 <- fitmodel$BUGSoutput$sims.list$yPred3

# t=2
plot(density(yPred1[,1,3]),main="",xlab="",ylab="",cex.axis=0.7,ylim=c(0,0.25),xlim=c(-4,16),lwd=1)
lines(density(yPred1[,2,3]),col=2,lwd=1)

lines(density(yPred2[,1,3]),main="",xlab="",ylab="",cex.axis=0.7,lty=2)
lines(density(yPred2[,2,3]),col=2,lty=2,lwd=1)

lines(density(yPred3[,1,3]),main="",xlab="",ylab="",cex.axis=0.7,lty=3)
lines(density(yPred3[,2,3]),col=2,lty=3,lwd=1)

lines(density(Y[,1]))
lines(density(Y[,2]))
lines(density(Y[,3]))
lines(density(Y[,4]))

#######################################################
# Title: Dependent Generalised Dirichlet Process Priors
# Author: William Barcella
# Description: Posterior densities of the regression 
# coefficients
#######################################################

par(mfrow=c(1,1),mar=c(2,2,2,2))

# Source from the posterior distributions 
bePred <- fitmodel$BUGSoutput$sims.list$bePred

# beta_1
plot(density(bePred[,1,1],adjust = 2),main="",xlab="",ylab="",cex.axis=0.7,xlim=c(-1,2),ylim=c(0,4))
lines(density(bePred[,2,1],adjust = 2),col=2,main="",xlab="",ylab="",cex.axis=0.7)
legend("topleft",
       c("Group 1","Group 2"), 
       col=c(1:2),lty=c(1,1),cex=0.7,bty = "n")

# beta_2
plot(density(bePred[,1,2]),main="",xlab="",ylab="",cex.axis=0.7,xlim=c(0.2,2.5),ylim=c(0,3.7))
lines(density(bePred[,2,2]),col=2,main="",xlab="",ylab="",cex.axis=0.7)

# beta_3
plot(density(bePred[,1,3]),main="",xlab="",ylab="",cex.axis=0.7,xlim=c(-0.5,2.5),ylim=c(0,3.9))
lines(density(bePred[,2,3]),col=2,main="",xlab="",ylab="",cex.axis=0.7)

# beta_4
plot(density(bePred[,1,4]),main="",xlab="",ylab="",cex.axis=0.7,xlim=c(-0.5,1.5),ylim=c(0,3.5))
lines(density(bePred[,2,4]),col=2,main="",xlab="",ylab="",cex.axis=0.7)

#######################################################
## Example 5: old Faithful geyser
##
## DPM for residuals in polynom regression

## SAMPLING MODEL
## y[i] = \sum b_j x[i]^j + e[i]  (for centered data)
## e[i] ~ F, i.i.d.

## PRIOR
## F = \int N(mu,V) dG(mu),
## G ~ DP(G0,M), G0(mu) = N(m0,V0), m0=0, V0=15^2

## complete conditionals
## p(ri | G), i=1..n
## r0h = sum(ri==h, i=n+1..k), h=1..H
## p(mh | r)
## p(wh | r)

## data
##   (i know it's bad programming to use so many global vars)
url <- "https://web.ma.utexas.edu/users/pmueller/bnp/R/regr/geyser/geyser.dat"
dta <- read.table(url)
plot(dta)
y.raw <- dta[,2]
yb <- mean(y.raw)      # center to zero mean
y <- y.raw-yb
e <- y                 # initialize residuals for polyn regression

x.raw <- dta[,1]       
xb <- mean(x.raw)
x <- x.raw-xb          # centered covariate
x2 <- x*x
x3 <- ifelse(x>0,x2*x,0)
X <- cbind(1,x,x2,x3)

n <- length(y)
### end data

## grids for plotting
xgrid <- seq(from=1.5-xb, to=5-xb, length=100)
xg2 <- xgrid*xgrid
xg3 <- ifelse(xgrid>0,xg2*xgrid,0)
ygrid <- seq(from=-35,to=35,length=100)
Xgrid <- cbind(1,xgrid,xg2,xg3)

## hyperparameters
## G0=N(0,15^2)
m0 <- 0        # base measure
V0 <- 15^2
V0i <- 1/V0
V <- 9         # kernel var
Vi <- 1/V

M <- 1         # total mass
H <- 50        # size of DP_K

## prior on poly regr
b0 <- rep(0,4)  ##   c(4.36, -9.11,-2.60, -0.85)
B0 <- 10*diag(4)  ## c(2,8,4,16))
B0i <- solve(B0)

# ##################################################################
# initialize clustering..
# ##################################################################


init.DPk <- function(H=10,e)
{ ## inital EDA estimate of G = sum_{h=1..10} w_h delta(m_h)
  ## returns:
  ##   list(mh,wh)
  ## use (mh,wh) to initialize the blocked Gibbs
  ## r[i]: latent cluster membership
  ## For i=n+1..k, summarize cluster memberships by h=1..H:
  ##     Sh0[h]: number of i=h, for i=n+1...k
  
  ## cluster data, and cut at height H=10, to get 10 clusters
  hc <- hclust(dist(e)^2, "cen")
  r  <- cutree(hc, k = H)
  ## record cluster specific means, order them 
  mh1 <- sapply(split(e,r),mean)    # cluster specific means == m_h
  wh1 <- table(r)/n
  idx <- order(wh1,decreasing=T)    # re-arrange in deceasing order
  mh <- mh1[idx]
  wh <- wh1[idx]
  return(list(mh=mh,wh=wh,r=r))
}   

rmvnorm <- function(m=0,S=1)
{
  p <- nrow(S)
  V <- chol(S)
  y <- rnorm(p,m=m)%*%V
  return(y)
}

sample.g <- function(e,beta)
{
  fit <- lm(e ~ x+x2+x3)
  m <- coef(fit)
  V <- vcov(fit)
  Vi <- solve(V)
  V1 <- solve(B0i+Vi)
  m1 <- V1 %*% (B0i%*%b0 + Vi%*%m)
  
  beta <- rmvnorm(m1,V1)
  ghat <- X%*%t(beta)
  e <- y - ghat
  g <- Xgrid %*% t(beta) + yb
  g <- c(g)
  return(list(e=e,ghat=ghat,g=g,beta=beta))
}

sample.b <- function(e,beta)
{
  fit <- lm(e ~ x+x2+x3)
  m <- coef(fit)
  V <- vcov(fit)
  Vi <- solve(V)
  V1 <- solve(B0i+Vi)
  m1 <- V1 %*% (B0i%*%b0 + Vi%*%m)
  
  beta <- rmvnorm(m1,V1)
  ghat <- X%*%t(beta)
  e <- y - ghat
  g <- Xgrid %*% t(beta) + yb
  g <- c(g)
  return(list(e=e,ghat=ghat,g=g,beta=beta))
}



# ##################################################################
#  Blocked GS
# ##################################################################


gibbs <- function(n.iter=1000,e=e)
{ ## returns fgrid: (n.iter x 9) matrix of imputed F
  ## klist:         posterior on k = latent sample size
  ## blist:         hyper-parameter of DP of Poi mixture
  
  G <- init.DPk(H,e)
  beta <- b0
  ## b <- a0/b0
  
  ## data structures to save imputed F ~ p(F | ...)
  fgrid    <- NULL      # residual dist
  ggrid    <- NULL      # reg mean function
  betalist <- NULL
  G0 <- dnorm(ygrid,m=0,sd=sqrt(var(e)))
  plot(ygrid,G0,type="l",bty="l",xlab="RESID",ylab="G",ylim=c(0,0.07))
  ## Gibbs
  for(iter in 1:n.iter){
    fit <- sample.g(e,beta)
    e <- fit$e
    beta <- fit$beta
    ghat <- fit$ghat
    g <- fit$g
    G$r  <- sample.r(G)   # 1. r_i ~ p(r_i | ...), i=1..n
    G$mh <- sample.mh(G,e)     # 2. m_h ~ p(m_h | ...), h=1..H
    G$vh <- sample.vh(G)       # 3. v_h ~ p(v_h | ...), h=1..H
    ## b <- sample.b(G)
    
    ## record draw F ~ p(F | th,sig,y) (approx)
    f   <- fbar.H(ygrid,G)
    dev.set(3)
    lines(ygrid,f,col=iter,lty=3)
    dev.set(2)
    lines(xgrid+xb,g,col=iter,lty=3,lwd=3)
    
    fgrid <- rbind(fgrid,f)
    ggrid <- rbind(ggrid,g)
    betalist <- rbind(betalist,beta)
  }
  ## add overall average (= posterior mean) to the plot
  fbar <- apply(fgrid,2,mean)
  gbar <- apply(ggrid,2,mean)
  lines(ygrid,fbar,lwd=3,col=2)
  
  return(list(fgrid=fgrid, beta=betalist, ggrid=ggrid))
}

plt.f <- function(fgrid,pltfgrid=T,pltsim=T,initial=T,cl=NULL)
{
  xgrid <- 0:8
  plot(0:8,seq(0,0.7,length=9),type="n",bty="l",xlab="COUNT",ylab="FREQ")
  lines(table(y)/n)  ##,xlim=c(0,8),bty="l",xlab="COUNT",ylab="FREQ")
  ## plot(table(y)/n,xlim=c(0,8),bty="l",xlab="COUNT",ylab="FREQ")
  if (pltsim){
    if (pltfgrid){
      nsim <- nrow(fgrid)
      if (initial)
        nsim0 <- floor(nsim/2)
      else
        nsim0 <- 1
      for(i in nsim0:nsim){
        if (is.null(cl))
          lines(xgrid,fgrid[i,],col=i,lty=3)
        else
          lines(xgrid,fgrid[i,],col=cl,lty=3)
      }
      ## add overall average (= posterior mean) to the plot
      fbar <- apply(fgrid[nsim0:nsim,],2,mean)
      lines(xgrid,fbar,lwd=3,col=1)
    }
  } #plotsim
  lines(table(y)/n,lwd=1)  ##,xlim=c(0,8),bty="l",xlab="COUNT",ylab="FREQ")
  xg <- as.numeric(names(table(y)))
  points(xg,table(y)/n,pch=19)  ##,xlim=c(0,8),bty="l",xlab="COUNT",ylab="FREQ")
}

sample.r <- function(G)
{ ## samle allocation indicators
  
  r <- rep(0,n)
  for(i in 1:n){
    ph <-   dnorm(y[i],m=G$mh,sd=sqrt(V))*G$wh # likelihood   * prior
    ## p(yi | ri=h) * w_h
    r[i] <- sample(1:H,1,prob=ph)
  }
  return(r)
}


sample.mh <- function(G,e)
{ ## sample mh ~ p(mh | ...)
  ##
  
  mh <- rep(0,H)     # initialize
  for(h in 1:H){
    Sh <- which(G$r==h) # Sh = {i: r[i]=h
    nh <- length(Sh)
    if (nh>0){
      ybar <- mean(e[Sh])
      V1 <- 1/(V0i+nh*Vi)
      m1 <- V1*(nh*Vi*ybar)
    } else {
      m1 <- 0
      V1 <- V0
    }
    mh[h] <- rnorm(1,m1,sd=sqrt(V1))
  }
  return(mh)
}



sample.vh <- function(G)
{## sample vh ~ p(vh | ...)
  ## returns: wh
  
  vh <- rep(0,H)  # initialize
  wh <- rep(0,H)
  V <-  1         # record prod_{g<h} (1-vh_h)
  for(h in 1:(H-1)){
    Ah <- which(G$r==h)
    nAh <- length(Ah)
    Bh <- which(G$r>h)
    nBh <- length(G$Bh)
    vh[h] <-  rbeta(1, 1+nAh, M+nBh)
    wh[h] <- vh[h]*V
    V <- V*(1-vh[h])
  }
  vh[H] <- 1.0
  wh[H] <- V
  return(wh)
}

fbar.H <- function(xgrid,G)
{ ## return a draw F ~ p(F | ...) (approx)
  
  fx <- rep(0,length(xgrid))
  for(h in 1:H)
    fx <- fx + G$wh[h]*dnorm(xgrid,m=G$mh[h],sd=sqrt(V))
  return(fx)
}


##################################################################
## RUN: execute the lines below
##      best to do it line by line...
##

ex <- function()
{
  dp <- gibbs(1000,e)
  fbar <- apply(dp$fgrid,2,mean)
  f <- fbar/sum(fbar)
  m <- sum(f*ygrid)
  g <- apply(dp$ggrid,2,mean)+m
  niter <- nrow(dp$ggrid)
  
  plot(x+xb,y+yb,xlab="DURATION",ylab="INTERVAL",bty="l",type="p",
       pch=19,xlim=range(xgrid)+xb)
  lines(xgrid+xb,g,type="l",lwd=3)
  idx <- (1:niter)%%5==0
  matlines(xgrid+xb, t(dp$ggrid[idx,])+m, type="l", col="grey")
  
  G0 <- dnorm(ygrid,m=0,sd=sqrt(V0))
  plot(ygrid-m,
       G0,type="l",bty="l",xlab="RESID",ylab="G",ylim=c(0,0.07))
  matlines(ygrid-m, t(dp$fgrid[idx,]), type="l",col="grey")
  lines(ygrid-m, fbar, type="l", lwd=3)
}


###################################################

url <- "https://web.ma.utexas.edu/users/pmueller/bnp/R/DP/eig121/EIG.txt"
dta <- read.table(url)

## Example 4 - Gene expression
## DPM
## implementing MCMC with finite DP
## Sec 2.4.6.

## MODEL:
## y_i | th_i ~ N(th_i, sig^2)
## th_i       ~ G and
## G          ~ DP(M, G0) with G0=N(0,4)
## hyperprior:
## 1/sig ~ Ga(a,b), a=1, b=1
##
## We use the usual notation for unique values ths[1..k] and
##        s_i = j if th_i = ths_j
## Denote with F(y) = \int N(y; th,sig) dG(th)
##        and  f(y) = pdf
##

## Posterior MCMC for DPM models
## (b) using the blocked Gibbs sampler
##         H
##     G = sum w_h delta(m_h)
##         h=1
##     recall: w_h = v_h (1-\sum_{g<h} w_g)
##             r_i = h iff th_i = m_h
##
##     We iterate over the following transition probabilities, sampling
##     from complete conditional posterior distributions:
##     1. r_i   ~ p(r_i   | ... ), i=1..n
##     2. m_h   ~ p(m_h   | ... ), h=1..H
##     3. v_h   ~ p(v_h   | ... ), h=1..H
##     4. sig   ~ p(sig   | ... )


## EIG 121 data
read.dta <- function()
{
  url <- "https://web.ma.utexas.edu/users/pmueller/bnp/R/DP/eig121/EIG.txt"
  X <- read.table(url)
  X = X[-1,]
  y <- X$V2
  y = as.numeric(y)
  y <- log( y[!is.na(y)] )  # log EIG121 expression
  n <- length(y)
  return(dta=list(y=y, n=n))
}


## hyperparameters
a <- 1;   b <- 1     # 1/sig ~ Ga(a,b)
m0 <- -3;  B0 <- 4    # G0 = N(m0,B0)
M <- 1
H <- 10

# ##################################################################
# initialize clustering..
# ##################################################################

init.DPk <- function()
{ ## inital EDA estimate of G = sum_{h=1..10} w_h delta(m_h)
  ## returns:
  ##   list(mh,wh)
  ## use (mh,wh) to initialize the blocked Gibbs
  
  ## cluster data, and cut at height H=10, to get 10 clusters
  hc <- hclust(dist(y)^2, "cen")
  r  <- cutree(hc, k = 10)
  ## record cluster specific means, order them 
  mh1 <- sapply(split(y,r),mean)    # cluster specific means == m_h
  wh1 <- table(r)/n
  idx <- order(wh1,decreasing=T)    # re-arrange in deceasing order
  mh <- mh1[idx]
  wh <- wh1[idx]
  return(list(mh=mh,wh=wh,r=r))
}   




# ##################################################################
# 2. Blocked GS
# ##################################################################


gibbs.H <- function(n.iter=500)
{
  
  G <- init.DPk()
  sig <- 0.11
  
  ## data structures to save imputed F ~ p(F | ...)
  xgrid <- seq(from= -10, to=2,length=50)
  fgrid <- NULL
  plot(density(y),xlab="X",ylab="Y",bty="l",type="l",
       xlim=c(-10, 2),ylim=c(0,0.4), main="")
  ## Gibbs
  for(iter in 1:n.iter){
    G$r <-  sample.r(G$wh,G$mh,sig)   # 1. r_i ~ p(r_i | ...), i=1..n
    G$mh <- sample.mh(G$wh,G$r,sig)   # 2. m_h ~ p(m_h | ...), h=1..H
    G$vh <- sample.vh(G$r)            # 3. v_h ~ p(v_h | ...), h=1..H
    th <- G$mh[G$r]                   # record implied th[i] = mh[r[i]]
    sig <- sample.sig(th)       # 4. sig ~ p(sig | ...)
    
    ## record draw F ~ p(F | th,sig,y) (approx)
    f   <- fbar.H(xgrid,G$wh,G$mh,sig)
    lines(xgrid,f,col=iter,lty=3)
    fgrid <- rbind(fgrid,f)
  }
  ## add overall average (= posterior mean) to the plot
  fbar <- apply(fgrid,2,mean)
  lines(xgrid,fbar,lwd=3,col=2)
  return(fgrid)
}

sample.r <- function(wh,mh,sig)
{ ## samle allocation indicators
  
  r <- rep(0,n)
  for(i in 1:n){
    ph <-   dnorm(y[i],m=mh,sd=sig)*wh # likelihood   * prior
    ## p(yi | ri=h) * w_h
    r[i] <- sample(1:H,1,prob=ph)
  }
  return(r)
}


sample.mh <- function(wh,r,sig)
{ ## sample mh ~ p(mh | ...)
  ##
  
  mh <- rep(0,H)     # initialize
  for(h in 1:H){
    if(any(r==h)){      # some data assigned to h-th pointmass
      Sh <- which(r==h) # Sh = {i: r[i]=h
      nh <- length(Sh)
      ybarh <- mean(y[Sh])
      varh   <- 1.0/(1/B0 + nh/sig^2)
      meanh  <- varh*(1/B0*m0 + nh/sig^2*ybarh)
    } else {            # no data assinged to h-th pointmass
      varh  <- B0       # sample from base measure
      meanh <- m0
    }
    mh[h] <- rnorm(1,m=meanh,sd=sqrt(varh))
  }
  return(mh)
}

sample.vh <- function(r)
{## sample vh ~ p(vh | ...)
  ## returns: wh
  
  vh <- rep(0,H)  # initialize
  wh <- rep(0,H)
  V <-  1         # record prod_{g<h} (1-vh_h)
  for(h in 1:(H-1)){
    Ah <- which(r==h)
    Bh <- which(r>h)
    vh[h] <-  rbeta(1, 1+length(Ah), M+length(Bh))
    wh[h] <- vh[h]*V
    V <- V*(1-vh[h])
  }
  vh[H] <- 1.0
  wh[H] <- V
  return(wh)
}

fbar.H <- function(xgrid,wh,mh,sig)
{ ## return a draw F ~ p(F | ...) (approx)
  
  fx <- rep(0,length(xgrid))
  for(h in 1:H)
    fx <- fx + wh[h]*dnorm(xgrid,m=mh[h],sd=sig)
  return(fx)
}


sample.sig <- function(th)
{ ## sample
  ##   sig ~ p(sig | ...)
  ## returns: sig
  
  s2 <- sum( (y-th)^2 )    # sum of squared residuals
  a1 <- a+0.5*n
  b1 <- b+0.5*s2
  s2.inv <- rgamma(1,shape=a1,rate=b1)
  return(1/sqrt(s2.inv))
}

plt.all <- function(fgrid,sim=T,dens=T)
{
  xgrid <- seq(from= -10, to=2,length=50)
  M <- nrow(fgrid)
  idx0 <- 21:M
  fgrid0 <- fgrid[idx0,]            # drop initial transient
  idx1 <- which(idx0 %% 5 == 0 )    # thin out for plotting
  fgrid1 <- fgrid[idx1,]
  fbar <- apply(fgrid0,2,mean)
  plot(xgrid,fbar,xlab="log(EIG121)", ylab="G",
       type="l",lwd=3,bty="l",ylim=c(0,0.35))
  if(sim){
    matlines(xgrid,t(fgrid1),col=1)
    ## matlines(xgrid,t(fgrid0),col=2)
    lines(xgrid,fbar,type="l",lwd=4,col="grey")
  }
  if (dens){
    lines(density(y),col="yellow",lty=2,lwd=4)
  }
}

plt.dta <- function()
{
  hist(y,main="",xlab="log(EIG121)",ylab="FREQ",prob=T)
}
##################################################################
## RUN:
## execute the commands below -- best line by line

ex <- function()
{
  dta <- read.dta()
  attach(dta)
  
  ## run MCMC
  fgrid <- gibbs.H()
  plt.dta()
  plt.all(fgrid)
  plot(y)
}






############################ Regresión vía DPM ############################ 

library(sn)

set.seed(123) 

beta <- c(1, 2, 1, 0.5)
n <- 100
x <- runif(n, min = -5, max = 5)
epsilon = rsn(n, xi=0, omega=10, alpha=5)
hist(epsilon, prob = T)
lines(density(epsilon), lwd = 2)

xvals = seq(-10,25, by = 0.1)
epssn = dsn(xvals, xi=-2, omega=10, alpha=5)
lines(xvals,epssn, col = "red", lwd = 2)
legend("bottomright", legend = c("Densidad Kernel","Densidad teórica"), 
       col = c("black","red"), lty = 1, lwd = 2)
y <- sapply(x, function(xi) sum(beta * c(1, xi, xi^2, xi^3))) + epsilon

data <- data.frame(x = x, y = y)
plot(data)

funcx = function(x) {
  1+2*x+x^2+0.5*x^3
}
xseq = seq(-5,5, by = 0.1)
lines(xseq,funcx(xseq), col = "blue", lwd = 2)

ml = lm(y ~ x, data = data)
ml=lm(formula = y ~ I(x) + I(x^2) + I(x^3), data = data)

b0 = ml$coefficients[1]
b1 = ml$coefficients[2]
b2 = ml$coefficients[3]
b3 = ml$coefficients[4]

funcx2 = function(b1,b2,b3,b4,x) {
  b0+b1*x+b2*x^2+b3*x^3
}

plot(data$x, data$y, main = "Ajuste", xlab = "x", ylab = "y")
lines(xseq,funcx2(b0,b1,b2,b3,xseq), col = "blue", lwd = 2)

e = y - (1 + 2*x + x^2 + 0.5*x^3)
hist(e, prob = T)
lines(density(e))

##################################

######### Algoritmo DPM #########

stick_breaking = function(K, alpha) {
  V = numeric(K)
  b = 1
  for(i in c(1:K)){
    a = rbeta(1, 1, alpha) 
    V[i] = a * b
    b = b * (1 - a)
  }
  return(V)
}

F_t = function(t, S, V) {
  sum(V[S <= t])
}

alpha = 5
K = 10000
n.iter = 20

t_values = seq(-10,25, by = 0.1)
F_values_matrix = matrix(0, nrow = length(t_values), ncol = n.iter)

set.seed(1)
for(i in c(1:n.iter)) {
  V = stick_breaking(K,alpha)
  S = rsn(K,xi = 0,omega = 1, alpha = 10) # Creencia F_0
  F_values_matrix[,i] = sapply(t_values, F_t, S = S, V = V)
}

plot(t_values, F_values_matrix[,1], type = "s", col = "grey", lwd = 2, 
     xlab = "t", ylab = "F(t)", 
     main = "Función de Distribución Acumulada F(t) vs Normal Asimétrica",
     xlim = c(-1,4))

for(i in c(1:n.iter)) {
  lines(t_values, F_values_matrix[, i], col = "grey", lwd = 2, type = "s")
}

grid()

F_0 = psn(t_values,xi = 0,omega = 1, alpha = 10)
lines(t_values, F_0, col = "red", lwd = 2)


legend("bottomright", legend = "F_0", 
       col = "red", lty = 1, lwd = 2)
text(x = -3, y = 0.95, labels = paste("alpha =", alpha), cex = 1.2, col = "black")


sample_from_F_first_curve = function(F_values, t_values) {
  u = runif(1) 
  t_sample = t_values[which.min(abs(F_values - u))]
  return(t_sample)
}

sampled_value_first_curve = matrix(0, nrow = K, ncol = n.iter)

for (i in c(1:n.iter)) {
  for (j in c(1:K)) {
    sampled_value_first_curve[j,i] = sample_from_F_first_curve(F_values_matrix[,i], t_values)
  }
}

x_n = matrix(0, nrow = K, ncol = n.iter)
for (i in c(1:n.iter)) {
  x_n[,i] = rsn(K,sampled_value_first_curve[,i], omega = 1, alpha = 10)
}

plot(density(x_n[,6]), col = "grey", lwd = 2, type = "s", lty = 2)

for(i in c(2:n.iter)) {
  lines(density(x_n[,i]), col = "grey", lwd = 2, type = "s", lty = 2)
}

grid()

######### Posteriori #########

n.iter = 20
ecdf_data = ecdf(e)
F_hat = ecdf_data(t_values)
n = length(e)
F_posterior_values_matrix = matrix(0, nrow = length(t_values), ncol = n.iter)

F_bar = (n/(n+alpha))*F_hat + (alpha/(n+alpha))*F_0

for(i in c(1:n.iter)) {
  V = stick_breaking(K,alpha+n)
  sampled_values = numeric(K)
  for (j in 1:K) {
    u = runif(1) 
    if (u < (n / (n + alpha))) {
      idx = sample(1:length(F_hat), 1, prob = diff(c(0, F_hat)))
      sampled_values[j] = t_values[idx]
    } else {
      sampled_values[j] = rsn(1,xi = 0,omega = 1, alpha = 10)
    }
  }
  F_posterior_values_matrix[,i] = sapply(t_values, F_t, S = sampled_values, V = V)
}

plot(t_values, F_posterior_values_matrix[,1], type = "s", col = "grey", lty = 2, lwd = 2, 
     xlab = "t", ylab = "F(t)", 
     main = "Función de Distribución Acumulada F(t) vs Mezcla de Normales")

for(i in c(1:n.iter)) {
  lines(t_values, F_posterior_values_matrix[, i], col = "grey", lwd = 2, type = "s")
}

sncdf = psn(t_values, xi=0, omega=10, alpha=5)
lines(t_values, sncdf, col = "red", lwd = 2)

text(x = -3, y = 0.95, labels = paste("alpha =", alpha), cex = 1.2, col = "black")

F_mean = rowMeans(F_posterior_values_matrix)

lines(t_values, F_mean, col = "blue", lwd = 2, type = "s")
lines(t_values, F_hat, col = "green", lwd = 2, type = "s")

sampled_value_first_curve = matrix(0, nrow = K, ncol = n.iter)

for (i in c(1:n.iter)) {
  for (j in c(1:K)) {
    sampled_value_first_curve[j,i] = sample_from_F_first_curve(F_posterior_values_matrix[,i], t_values)
  }
}

x_n = matrix(0, nrow = K, ncol = n.iter)
for (i in c(1:n.iter)) {
  x_n[,i] = rnorm(K,sampled_value_first_curve[,i], sd = 1)
}

####

densidades = apply(x_n, 2, density)

x_vals = densidades[[1]]$x

densidades_vals = sapply(densidades, function(d) approxfun(d$x, d$y)(x_vals))

densidades_vals_clean = densidades_vals[, !apply(is.na(densidades_vals), 2, any)]

densidad_promedio = rowMeans(densidades_vals_clean)

percentil_2_5 = apply(densidades_vals_clean, 1, function(x) quantile(x, 0.025))
percentil_97_5 = apply(densidades_vals_clean, 1, function(x) quantile(x, 0.975))

### Ploteando ###

plot(density(x_n[,1]), col = "grey", lwSd = 2, type = "s", lty = 2, ylim = c(0,0.17))

for(i in c(2:n.iter)) {
  lines(density(x_n[,i]), col = "grey", lwd = 2, type = "s", lty = 2)
}
lines(density(e), col = "blue", lwd = 2, type = "s")
lines(x_vals, densidad_promedio, col='red', lwd=2) 

polygon(c(x_vals, rev(x_vals)), c(percentil_2_5, rev(percentil_97_5)), 
        col=rgb(0, 0, 1, 0.3), border=NA) 
legend("topright", legend = c("Densidad de los residuos", "Densidad promedio"), 
       col = c("blue","red"), lty = 1, lwd = 2)
grid()

############## PROCESO GAUSSIANO ##############

###### Dividir la base en datos de entrenamiento y testeo ######

### Datos


plot(data)
data = data %>%
  arrange(x)
x_values = data$x*100
d = fields::rdist(data$x)
phi = 0.1855
sigma_g = 204.6264
K = sigma_g^2*exp(-(phi^2)*(d^2))


dot_product <- function(x, y) {
  return(x * y)  # Producto escalar entre dos valores
}

# Función para calcular la matriz de productos entre todos los puntos
dotMatrix <- function(data) {
  n <- length(data)  # Número de puntos en el vector
  dot_product_matrix <- matrix(0, nrow = n, ncol = n)  # Inicializa la matriz de productos
  
  # Calcular el producto entre cada par de puntos
  for (i in 1:n) {
    for (j in 1:n) {
      # Calcular el producto entre el punto i y el punto j
      prod <- dot_product(data[i], data[j])
      
      # Asignar el producto en la posición (i, j)
      dot_product_matrix[i, j] <- prod
    }
  }
  
  return(dot_product_matrix)
}


K = (1*dotMatrix(data$x)+50)^3

g = mvtnorm::rmvnorm(5, sigma=K)
matplot(data$x, t(g), 
        type="l", 
        ylab="y")
set.seed(1)
g = mvtnorm::rmvnorm(1, sigma=K) # GP
matplot(data$x, t(g), 
        type="l", 
        ylab="y")

data = data.frame(x_values = sort(x_values),
                  x = data$x,
                  y = data$y, 
                  gp = c(g))

ggplot(data, aes(x = x_values, y = y)) +
  geom_point(aes(colour = "Simulated Data")) +
  geom_line(aes(x = x_values, y = gp, colour = "Gaussian Process")) +
  labs(colour = "")


model_string <- '
model{

  gp ~ dmnorm(mu,Sigma.inv)
  Sigma.inv <- inverse(Sigma)
  
  for(i in 1:n_obs)
  {
    mu[i] <- 0
    Sigma[i,i] <- sigma_g^2 + 0.00001
    for(j in (i+1):n_obs) {
    Sigma[i,j] <- sigma_g^2*exp(-(phi^2)*(d[i,j]^2))
    Sigma[j,i] <- Sigma[i,j]
    }
      
    y[i]~dnorm(gp[i],sigma_y^-2)
  }
  
  sigma_g ~ dt(0,10^-2,1)T(0,)
  phi ~ dt(0,4^-2,1)T(0,)
  sigma_y ~ dt(0,10^-2,1)T(0,)
}'

### get data and estimation years
x <- data$x
y <- data$y
n_obs <- length(x)
dist <- fields::rdist(x) 


###The required data
data_list <- list(y = y,
                  n_obs=n_obs,
                  d = d) 

##parameters to save
params = c("sigma_g", "phi", "sigma_y")  

inits1 = list(.RNG.name = "base::Mersenne-Twister", .RNG.seed = 1)
inits2 = list(.RNG.name = "base::Mersenne-Twister", .RNG.seed = 2)
inits3 = list(.RNG.name = "base::Mersenne-Twister", .RNG.seed = 3)
mod = jags.model(textConnection(model_string), data = data_list, n.chains = 3,
                 inits = list(inits1, inits2, inits3))
update(mod, 1000)

mod_sim = coda.samples(model = mod, variable.names = params, n.iter = 10000)
mod_csim = as.mcmc(do.call(rbind, mod_sim))

summary(mod_csim)
gelman.diag(mod_sim)
autocorr.diag(mod_sim)
effectiveSize(mod_sim)

plot(mod_csim)

########### Predicciones ###########

n_pred <- 50
x_star <- seq(min(x), max(x), length.out = n_pred)

par_est = colMeans(mod_csim)
par_est = data.frame(phi = par_est[1], sigma_g = par_est[2], sigma_y = par_est[3])

Sigma <-  par_est$sigma_y*2 * diag(n_obs) + par_est$sigma_g^2*exp(-(par_est$phi^2)*rdist(x,x)^2)
Sigma_star <- par_est$sigma_g^2*exp(-(par_est$phi^2)*rdist(x_star,x)^2) 
Sigma_star_star <- par_est$sigma_g^2*exp(-(par_est$phi^2)*rdist(x_star,x_star)^2)

pred_mean <- Sigma_star %*% solve(Sigma, y)
pred_var <- Sigma_star_star - Sigma_star %*% solve(Sigma, t(Sigma_star))

pred_res <- tibble(pred_mean = pred_mean, 
                   x_values = x_star,
                   lwr_95 = pred_mean - 1.96 * sqrt(diag(pred_var)), 
                   upr_95 = pred_mean + 1.96 * sqrt(diag(pred_var)))

x_values = x*100
ggplot(pred_res, aes(x = x_values, y = pred_mean)) +
  geom_line() +
  geom_ribbon(aes(ymin = lwr_95, ymax = upr_95), alpha = 0.3) +
  geom_point(data = data, aes(x = x, y = y), alpha = 0.2) +
  labs(y = "y", colour = "", fill = "") +
  theme_bw() 

############ Agregar un solo punto ###########

new_point = 4  # Un nuevo punto de entrada (x)
new_observation <- pred_mean[which.min(abs(x_star - new_point))];new_observation 
new_point_var <- pred_var[which.min(abs(x_star - new_point)), which.min(abs(x_star - new_point))]

ggplot(pred_res, aes(x = x_values, y = pred_mean)) +
  geom_line() +
  geom_ribbon(aes(ymin = lwr_95, ymax = upr_95), alpha = 0.3) +
  geom_point(data = data, aes(x = x, y = y), alpha = 0.2) +
  annotate("point", x = new_point, y = new_observation, color = "red", size = 3) +  # Usar annotate() para el nuevo punto
  annotate("text", x = new_point, y = new_observation, label = paste("(", round(new_point, 2), ",", round(new_observation, 2), ")", sep=""), vjust = -1) +  # Etiqueta del nuevo punto
  labs(y = "y", colour = "", fill = "") +
  theme_bw()


set.seed(1)
simulated_values <- rnorm(1000, mean = new_observation, sd = sqrt(new_point_var))

lower_95 <- new_observation - 1.96 * sqrt(new_point_var)
upper_95 <- new_observation + 1.96 * sqrt(new_point_var)

ggplot() +
  geom_density(aes(x = simulated_values), fill = "blue", alpha = 0.5) +  # Distribución simulada
  geom_vline(xintercept = new_observation, color = "skyblue", linetype = "dashed", size = 1) +  # Media
  geom_vline(xintercept = lower_95, color = "red", linetype = "dashed", size = 1) +  # Límite inferior 95%
  geom_vline(xintercept = upper_95, color = "red", linetype = "dashed", size = 1) +  # Límite superior 95%
  annotate("text", x = lower_95, y = 0.05, label = paste("Lower 95%: ", round(lower_95, 2)), color = "green", vjust = -1) +  # Etiqueta inferior
  annotate("text", x = upper_95, y = 0.05, label = paste("Upper 95%: ", round(upper_95, 2)), color = "green", vjust = -1) +  # Etiqueta superior
  annotate("text", x = new_observation, y = 0.1, label = paste("Mean: ", round(new_observation, 2)), color = "red", vjust = -1) +  # Etiqueta media
  labs(title = "Distribución Condicional en el Punto con Intervalo de Confianza al 95%",
       x = "Valor predicho", y = "Densidad") +
  theme_bw()

#######################################################

set.seed(123)
n2 <- 1500
x2 <- runif(n2, min = 0, max = 5)
y2 <- sin(3 * x2) + log(x2 + 1) + rnorm(n2, sd = 0.5)
data2 <- data.frame(x = x2, y = y2)
plot(x2,y2)
plot(x,y)

# Cortes en x cada 0.5
breaks <- seq(-5, 5, by = 1)
data <- data %>%
  mutate(
    x_group = cut(x, breaks = breaks, include.lowest = TRUE)
  )

# Calcular centros de cada grupo
group_centers <- data %>%
  group_by(x_group) %>%
  summarize(x_center = mean(range(x)), .groups = "drop")

# Unir centros al dataset
data_grp <- data %>%
  left_join(group_centers, by = "x_group") %>%
  group_by(x_group, x_center) %>%
  group_split()

# Estimar densidades condicionales f_x(y)
dens_data <- map_dfr(data_grp, function(df) {
  dens <- density(df$y, from = min(y), to = max(y), n = 100)
  tibble(
    x_center = df$x_center[1],
    y = dens$x,
    fx_y = dens$y
  )
})

# Crear gráfico 3D
p <- plot_ly()

# Añadir curvas de densidad cada 0.5 unidades de x
for (grp in unique(dens_data$x_center)) {
  curve_df <- dens_data %>% filter(x_center == grp)
  p <- p %>%
    add_trace(data = curve_df,
              x = ~x_center, y = ~y, z = ~fx_y,
              type = 'scatter3d', mode = 'lines',
              line = list(color = 'black', width = 3),
              showlegend = FALSE)
}

# Añadir puntos (x, y) en plano z = 0
p <- p %>%
  add_trace(data = data,
            x = ~x, y = ~y, z = 0,
            type = 'scatter3d', mode = 'markers',
            marker = list(size = 2, color = 'gray'),
            name = "Datos") %>%
  layout(scene = list(
    xaxis = list(title = "x", range = c(-5, 5)),
    yaxis = list(title = "y"),
    zaxis = list(title = "fₓ(y)")
  ))

# Mostrar gráfico
p

########### Distribución de un punto ##########

point_indices <- seq(1, n_pred)

anim_data <- data.frame()

x_range <- seq(-40, 100, by = 0.01)
for (point_index in point_indices) {
  mean_point <- pred_mean[point_index]
  var_point <- pred_var[point_index, point_index]
  sd_point <- sqrt(var_point)

  density_values <- dnorm(x_range, mean = mean_point, sd = sd_point)

  anim_data <- rbind(anim_data, data.frame(x = x_range, density = density_values, point_index = point_index, x_test = x_star[point_index]))
}

anim <- ggplot(anim_data, aes(x = x, y = density, group = point_index)) +
  geom_line(color = "blue", size = 1) +
  geom_vline(aes(xintercept = pred_mean[point_index]), color = "red", linetype = "dashed", size = 1) +
  labs(
    title = "Distribución de los datos de testeo",
    x = "Valor de y", y = "Densidad"
  ) +
  theme_minimal() +
  transition_states(point_index, transition_length = 2, state_length = 1) +  # Animación por `point_index`
  enter_fade() + exit_fade()

anim_save("animation3.gif", animation = anim, renderer = gifski_renderer())

</pre>
